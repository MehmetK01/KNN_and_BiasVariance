---
title: "K-nearest Neighbours (from scratch) and the Bias-Variance trade-off"
author: "Mehmet"
date: "2024-11-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

We will use a simulated regression model to estimate the bias and variance, and then validate our formula. Our simulation is based on the following model:

\[
Y = \exp(\beta^T x) + \epsilon
\]

where \(\beta = \begin{bmatrix} 0.5 \\ -0.5 \\ 0 \end{bmatrix}\), \(X\) is generated uniformly from \([0,1]^3\), and \(\epsilon\) follows i.i.d. standard Gaussian. We will generate some training data and our goal is to predict a testing point at \(x_0 = \begin{bmatrix} 1 \\ -0.75 \\ -0.7 \end{bmatrix}\).

The true mean of \(Y\) at the testing point \(x_0\) is

\[
\exp(\beta^T x_0) \approx 0.852
\]


```{r}
  # true beta
  b = c(0.5, -0.5, 0)
  
  # testing point
  x0 = c(1, -0.75, -0.7)
  
  # true mean
  f0 = exp(sum(b * x0))
  f0
```

```{r}
# set seed
  set.seed(432)
  
  # generate training data
  n = 100
  p = 3
  X = matrix(runif(3 * n), ncol = 3)
  Y = exp(X %*% b) + rnorm(n)
  
  # find the closest k points
  k = 21
  distance = rowSums(sweep(X, 2, x0, "-")^2)
  index = order(distance)[1:k]

  # prediction
  mean(Y[index])
```

```{r}
  # validate that with the kknn package 
  library(FNN)
  knn.fit = knn.reg(train = X, test = data.frame(x = t(x0)), y = Y, k = k, algorithm = "brute")
  knn.fit$pred
```

The prediction is \(1.2858967\). This matches the result from the `knn.reg()` function.

Now we will estimate the bias of the KNN model for predicting \(x_0\). To estimate the bias, we need a simulation study. To do this, we perform a simulation that repeats, say, 1000 times. The bias of a model is defined as 

\[
\mathbb{E}[\hat{f}(x_0)] - f(x_0).
\]

We will use the same sample size \(n = 100\) and the same \(k = 21\).

```{r}
  # simulation
  nsim = 1000
  fhat = rep(NA, nsim)
  
  for (i in 1:nsim) {
    # generate training data
    X = matrix(runif(3 * n), ncol = 3)
    Y = exp(X %*% b) + rnorm(n)
    
    # find the closest k points
    distance = rowSums(sweep(X, 2, x0, "-")^2)
    index = order(distance)[1:k]
    
    # predicted label
    fhat[i] = mean(Y[index])
  }
  
  # bias
  bias = mean(fhat) - f0
  bias
```

The estimated bias of the KNN model for predicting \(x_0\) is \(-1.17298\).

Now, we estimate the variance of this model. The variance of a model is defined as 

\[
\mathbb{E}\left[(\hat{f}(x_0) - \mathbb{E}[\hat{f}(x_0)])^2\right].
\]



```{r}
  # variance
  variance = var(fhat)
  variance
```

The estimated variance of the KNN model for predicting \(x_0\) is \(0.0486988\).

Based on the bias-variance decomposition, the irreducible error is the variance of the noise term, which is \(1\). Based on our previous simulation, the Bias\(^2\) error is \(1.375882\) and the variance error is \(0.0486988\). Therefore, the prediction error is

\[
\text{Prediction Error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error} = 1.375882 + 0.0486988 + 1 = 2.424581.
\]

```{r}
  # prediction error
  1 + bias^2 + variance
```

To validate this theoretical result, let's turn to our simulation study again:

```{r}
  # simulation
  error = rep(NA, nsim)
  
  for (i in 1:nsim) {
    # generate training data
    X = matrix(runif(3 * n), ncol = 3)
    Y = exp(X %*% b) + rnorm(n)
    
    # find the closest k points
    distance = rowSums(sweep(X, 2, x0, "-")^2)
    index = order(distance)[1:k]
    
    # predicted label
    y_pred = mean(Y[index])
    
    # generate testing data
    Y0 = exp(x0 %*% b) + rnorm(1)
    
    # prediction error
    error[i] = (Y0 - y_pred)^2
  }
  
  # prediction error
  mean(error)
```

## classification

K-Nearest Neighbors for Multi-class Classification

The MNIST dataset of handwritten digits is one of the most popular imaging data during the early times of machine learning development. Many machine learning algorithms have pushed the accuracy to over 99% on this dataset. The dataset is stored in an online repository in CSV format, https://pjreddie.com/media/files/mnist_train.csv. We will download the first 2500 observations of this dataset from an online resource using the following code. The first column is the digits. The remaining columns are the pixel values. After we download the dataset, we save it to our local disk so we do not have to re download the data in the future.

```{r}
  # inputs to download file
  fileLocation <- "https://pjreddie.com/media/files/mnist_train.csv"
  numRowsToDownload <- 2500
  localFileName <- paste0("mnist_first", numRowsToDownload, ".RData")
  
  # download the data and add column names
  mnist <- read.csv(fileLocation, nrows = numRowsToDownload)
  numColsMnist <- dim(mnist)[2]
  colnames(mnist) <- c("Digit", paste("Pixel", seq(1:(numColsMnist - 1)), sep = ""))
  
  # save file
  # in the future we can read in from the local copy instead of having to redownload
  save(mnist, file = localFileName)
  
  # you can load the data with the following code
  load(file = localFileName)
```


We predict the label of the 123rd observation using the first 100 observations as the input training data matrix. We use \(K = 10\).

```{r}
# Function Name: euclidean_distance
# Function Usage: to calculate the Euclidean distance between two vectors
# Input 1: vec1, a vector of length p
# Input 2: vec2, a vector of length p
# Output: euclDist, a numeric
euclidean_distance <- function(vec1, vec2){
  # calculate Euclidean distance
  euclDist <- sqrt(sum((vec1 - vec2)^2))
  
  # return Euclidean distance
  return(euclDist)
}

# Function Name: manhattan_distance
# Function Usage: to calculate the Manhattan distance between two vectors
# Input 1: vec1, a vector of length p
# Input 2: vec2, a vector of length p
# Output: manhDist, a numeric
manhattan_distance <- function(vec1, vec2){
  # calculate Manhattan distance
  manhDist <- sum(abs(vec1 - vec2))
  
  # return Manhattan distance
  return(manhDist)
}

# Function Name: euclidean_distance_all
# Function Usage: to calculate the Euclidean distance between a vectors and
# all the row vectors in a matrix
# Input 1: vec1, a vector of length p
# Input 2: mat1_X, a matrix of dimension n x p
# Output: output_euclDistVec, a vector of length n
euclidean_distance_all <- function(vec1, mat1_X){
  # number of rows
  numRows <- dim(mat1_X)[1]
  output_euclDistVec <- rep(NA, times = numRows)
  
  # calculate Euclidean distance between vec1 and all row vectors of mat1_X
  for(i in 1:numRows){
    output_euclDistVec[i] <- euclidean_distance(vec1 = vec1, vec2 = mat1_X[i,])
  }
  
  # return Euclidean distances
  return(output_euclDistVec)
}

# Function Name: manhattan_distance_all
# Function Usage: to calculate the Manhattan distance between a vectors and
# all the row vectors in a matrix
# Input 1: vec1, a vector of length p
# Input 2: mat1_X, a matrix of dimension n x p
# Output: output_manhattanDistVec, a vector of length n
manhattan_distance_all <- function(vec1, mat1_X){
  # number of rows
  numRows <- dim(mat1_X)[1]
  output_manhattanDistVec <- rep(NA, times = numRows)
  
  # calculate Manhattan distance between vec1 and all row vectors of mat1_X
  for(i in 1:numRows){
    output_manhattanDistVec[i] <- manhattan_distance(vec1 = vec1, vec2 = mat1_X[i,])
  }
  
  # return Manhattan distances
  return(output_manhattanDistVec)
}

# Function Name: my_KNN
# Function Usage: to calculate the K nearest neighbors for a vector,
# the potential neighbors are all the row vectors in a matrix
# Input 1: vec1, a vector of length p
# Input 2: mat1_X, a matrix of dimension n x p
# Input 3: mat1_Y, a vector of length n
# Input 4: K, a positive integer
# Input 5: euclDistUsed, a Boolean
# Output: output_knnMajorityVote, a list of length 2
my_KNN <- function(vec1, mat1_X, mat1_Y, K, euclDistUsed){
  # number of row vectors
  numRows <- dim(mat1_X)[1]
  
  # check if we are calculating Euclidean distances or Manhattan distances
  if(euclDistUsed){
    distancesToVecs <- euclidean_distance_all(vec1 = vec1, mat1_X = mat1_X)
  } else{
    distancesToVecs <- manhattan_distance_all(vec1 = vec1, mat1_X = mat1_X)
  }
  
  # sort the distances
  distancesToVecsSorted <- sort(distancesToVecs, decreasing = FALSE)
  
  # extract K-smallest value
  K_th_smallest_distance <- distancesToVecsSorted[K]
  
  # extract indices from unsorted vector where value is <= K_th_smallest_distance
  K_closest_neighbor_indices <- (1:numRows)[which(distancesToVecs <= K_th_smallest_distance)]
  
  # extract labels corresponding to those indices
  K_closest_neighbors_labels <- mat1_Y[K_closest_neighbor_indices]
  
  # majority vote
  K_closest_neighbors_labels_majority_vote <- as.numeric(names(sort(table(K_closest_neighbors_labels),
                                                                    decreasing = TRUE)[1]))
  
  # store results in list
  output_knnMajorityVote <- list("K Nearest Neigbor Labels" = K_closest_neighbors_labels,
                                 "Majority Vote" = K_closest_neighbors_labels_majority_vote)
  
  # return list
  return(output_knnMajorityVote)
}

# divide dataset into training dataset
q2_1_training_data_size <- 100
q2_1_train <- mnist[1:q2_1_training_data_size,]
q2_1_train_X <- as.matrix(q2_1_train[,which(colnames(q2_1_train) != "Digit")])
q2_1_train_Y <- as.matrix(q2_1_train$Digit)
colnames(q2_1_train_Y) <- "Digit"

# parameters for KNN
q2_1_K <- 10

# extract vector for 123rd observation
obs_123_X <- mnist[123, which(colnames(mnist) != "Digit")]
obs_123_Y <- mnist[123, "Digit"]

# run KNN for 123rd observation with Euclidean distance
obs_123_eucl <- my_KNN(vec1 = obs_123_X,
                       mat1_X = q2_1_train_X,
                       mat1_Y = q2_1_train_Y,
                       K = q2_1_K,
                       euclDistUsed = TRUE)
print(obs_123_eucl)
```

```{r}
# run KNN for 123rd observation with Manhattan distance
obs_123_manhattan <- my_KNN(vec1 = obs_123_X,
                            mat1_X = q2_1_train_X,
                            mat1_Y = q2_1_train_Y,
                            K = q2_1_K,
                            euclDistUsed = FALSE)
print(obs_123_manhattan)

# true label
print(paste0("True label for 123rd observation: ", obs_123_Y))
```

The predicted label when we perform KNN with \(K = 10\) and a majority vote is \(7\) when using either Euclidean or Manhattan distance. Since the true label is \(7\), the prediction is correct.
